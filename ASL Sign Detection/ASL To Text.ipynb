{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a33de53",
   "metadata": {},
   "source": [
    "### <center><h1> CAPSTONE PROJECT </h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2c2034",
   "metadata": {},
   "source": [
    "# <center><h1>ASL SIGN DETECTION</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0230ec2",
   "metadata": {},
   "source": [
    "***What's ASL ?***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f64793",
   "metadata": {},
   "source": [
    "*American Sign Language is a Visual Language that is predominantly used by the Deaf Communities in North America and Anglophone Canada. It uses both manual and non-manual (emotional) cues to communicate with others. In this project we are going to solely focus on the manual subsystem of ASL. More specifically the English Alphabets.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7787c00",
   "metadata": {},
   "source": [
    "**Workflow of the Project**\n",
    "   - Importing necessary libraries\n",
    "   - Creating a function to save frames from a live webcam feed in a desired format\n",
    "   - Preparation of Training Data\n",
    "   - Model construction\n",
    "   - Training the model\n",
    "   - Using the model to predict ASL hand signs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da764868",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29defea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow.keras\n",
    "from tensorflow import keras\n",
    "from IPython.display import clear_output\n",
    "from IPython.utils import io"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a68fe13",
   "metadata": {},
   "source": [
    "Directory where the Images Captured using webcam will be saved \n",
    "\n",
    "*Change it to a directory of your convinience*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efad9560",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SAVE_DIR = \"C:/Jupyter/Capstone Project - ASL to Text/ImagesFromROI/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5399d503",
   "metadata": {},
   "source": [
    "Directory which will contain the Training Images according to alphabet. Sub-directories of alphabets are already present \n",
    "\n",
    "*Change it to a directory of your convinience*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e32a765",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR = \"C:/Jupyter/Capstone Project - ASL to Text/TrainingImages/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b6e600",
   "metadata": {},
   "source": [
    "Defining the classes of images that we wish to classify. *As the letters J and Z require motion we are excluding them here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b06c8825",
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORIES = ['A','B','C','D','E','F','G','H','I','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5ff3a2",
   "metadata": {},
   "source": [
    "### Function To Save Frames from Webcam within our Region of Intrest [ROI]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e3a0ba",
   "metadata": {},
   "source": [
    "This integer variable is used as a counter to keep track of the images saved, and to make sure files don't have the same name and get overwritten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb3b899f",
   "metadata": {},
   "outputs": [],
   "source": [
    "currentFrame = 0 #Intializing counter to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa4f7640",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0) # Defining VideoCapture Object with value 0, which means it will use the webcam\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read() # Reading each frame from the webcam and storing it in a variable called frame.\n",
    "    \n",
    "    mirror = cv2.flip(frame, 1) # Flipping each frame so that the video feed resembles a mirror  \n",
    "    \n",
    "    fh, fw = mirror.shape[:2] # Getting the frame's width and height\n",
    "    \n",
    "    rois = int(fh/1.7) # Defining the side of the ROI as being half the length of the height of the frame\n",
    "    \n",
    "    cropImg = mirror[0:rois, fw-rois:fw] # Cropping out the part necessary for the ROI\n",
    "    \n",
    "    grey = cv2.cvtColor(cropImg, cv2.COLOR_BGR2GRAY) # Converting the BGR image of the ROI to Greyscale (B&W) \n",
    "    \n",
    "    value = (11, 11) # Setting the Blur Kernel size\n",
    "    \n",
    "    blurred = cv2.GaussianBlur(grey, value, 0) # Blurring the Greyscale Image\n",
    "    \n",
    "    _, thresh = cv2.threshold(blurred, 127, 255, cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU) # Applying Thresholding \n",
    "    \n",
    "    third_axis = np.repeat(thresh[...,np.newaxis], 3, -1) # B&W images don't have a 3rd axis. So adding it manually.\n",
    "    \n",
    "    mirror[0:rois, fw-rois:fw] = third_axis # Overlaying the thresholded image on our webcam feed \n",
    "    \n",
    "    cv2.imshow(\"Webcam\", mirror) # Showing the video to user\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'): # If the key 'q' is pressed the live session will terminate\n",
    "        break\n",
    "        \n",
    "    if cv2.waitKey(1) & 0xFF == ord('s'): # If the key 's' is pressed the image in the ROI at that time will get saved\n",
    "        cv2.imwrite(IMG_SAVE_DIR+'frame'+str(currentFrame)+'.jpg', third_axis)\n",
    "        print('Saved Pic '+str(currentFrame))\n",
    "        currentFrame+=1\n",
    "    \n",
    "cap.release()              \n",
    "cv2.destroyAllWindows() # Terminating the session if 'q' is pressed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4a83f3",
   "metadata": {},
   "source": [
    "### Preparing Training Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7520414e",
   "metadata": {},
   "source": [
    "Creating empty directory to store training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2c1b951",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainImgs = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bc9700",
   "metadata": {},
   "source": [
    "Recursive function that reads in image using OpenCv and Resizes it and stores the image array in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24a41fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat in CATEGORIES:\n",
    "    path=os.path.join(TRAIN_DIR,cat) # Specifying the directory to take images from for each alphabet\n",
    "    class_num = CATEGORIES.index(cat) # Using the index of the CATEGORIES list to assign a label to each image\n",
    "    for img in os.listdir(path): # FOR loop which appends read images to the Training List\n",
    "        img=cv2.imread(TRAIN_DIR+cat+'/'+img)\n",
    "        resizedimg = cv2.resize(img, (224,224), interpolation= cv2.INTER_CUBIC)\n",
    "        trainImgs.append([resizedimg,class_num])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e540133",
   "metadata": {},
   "source": [
    "Checking if the images are stored correctly. *Press any key to exit from saved image*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1638ad4f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cv2.imshow('Random', trainImgs[787][0])\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3001d143",
   "metadata": {},
   "source": [
    "Shuffling the Train Data so that the Neural Network doesn't develop a pattern recognition system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8aa81407",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "random.shuffle(trainImgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f8735c",
   "metadata": {},
   "source": [
    "Seperating the Images and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c306825",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "for item in trainImgs:\n",
    "    X.append(item[0])\n",
    "    y.append(item[1])\n",
    "X=np.array(X)\n",
    "y=np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f449f5",
   "metadata": {},
   "source": [
    "Normalizing the Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23ccc225",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xnor = X/255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c388823",
   "metadata": {},
   "source": [
    "Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3e3427e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(Xnor, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308e3521",
   "metadata": {},
   "source": [
    "### Model Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6c67ee",
   "metadata": {},
   "source": [
    "Specifying URL where MobileNetV2 resides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1575ef3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mobilenet_v2 = \"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e31bc80",
   "metadata": {},
   "source": [
    "Specifying the Input Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8fea31ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "mobile_net_layers = hub.KerasLayer(mobilenet_v2, input_shape=(224,224,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b22919",
   "metadata": {},
   "source": [
    "Making sure that the Inner Layers remain same as we don't want to change any pre-training in the CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8bdd41f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mobile_net_layers.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb65f6d",
   "metadata": {},
   "source": [
    "Constructing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e54c8a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "  mobile_net_layers,\n",
    "  tf.keras.layers.Dropout(0.3),\n",
    "  tf.keras.layers.Dense(24,activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ee4f83",
   "metadata": {},
   "source": [
    "Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3d6f489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " keras_layer (KerasLayer)    (None, 1280)              2257984   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1280)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 24)                30744     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,288,728\n",
      "Trainable params: 30,744\n",
      "Non-trainable params: 2,257,984\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8045c972",
   "metadata": {},
   "source": [
    "Compiling the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7e96a2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286e3681",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ff4bbb9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "111/111 [==============================] - 77s 657ms/step - loss: 1.2384 - accuracy: 0.6679 - val_loss: 0.3186 - val_accuracy: 0.9576\n",
      "Epoch 2/4\n",
      "111/111 [==============================] - 64s 577ms/step - loss: 0.3138 - accuracy: 0.9231 - val_loss: 0.1782 - val_accuracy: 0.9623\n",
      "Epoch 3/4\n",
      "111/111 [==============================] - 61s 548ms/step - loss: 0.1872 - accuracy: 0.9552 - val_loss: 0.1455 - val_accuracy: 0.9543\n",
      "Epoch 4/4\n",
      "111/111 [==============================] - 58s 526ms/step - loss: 0.1466 - accuracy: 0.9620 - val_loss: 0.0887 - val_accuracy: 0.9868\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2c2d724b190>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=4, validation_data=(X_test, y_test))      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a729e120",
   "metadata": {},
   "source": [
    "As we can see the model has good accuracy. So I will use the entire data to train the model this time instead of just the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bdf3498c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "158/158 [==============================] - 68s 403ms/step - loss: 0.9605 - accuracy: 0.7416\n",
      "Epoch 2/5\n",
      "158/158 [==============================] - 61s 387ms/step - loss: 0.2316 - accuracy: 0.9394\n",
      "Epoch 3/5\n",
      "158/158 [==============================] - 61s 389ms/step - loss: 0.1350 - accuracy: 0.9674\n",
      "Epoch 4/5\n",
      "158/158 [==============================] - 61s 386ms/step - loss: 0.1011 - accuracy: 0.9748\n",
      "Epoch 5/5\n",
      "158/158 [==============================] - 59s 376ms/step - loss: 0.0785 - accuracy: 0.9819\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2c2df9389d0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "  mobile_net_layers,\n",
    "  tf.keras.layers.Dropout(0.3),\n",
    "  tf.keras.layers.Dense(24,activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "model.fit(Xnor, y, epochs=5)      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85531e1",
   "metadata": {},
   "source": [
    "Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5368044e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('MobileNetV2TrainedOnBgSubtraction.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdb5346",
   "metadata": {},
   "source": [
    "Loading the saved model *(This step was done for ease of use so that I didn't need to train the model everytime I wanted to make some changes)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2a58d3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=keras.models.load_model('MobileNetV2TrainedOnBgSubtraction.h5',custom_objects={'KerasLayer': hub.KerasLayer})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ce423e",
   "metadata": {},
   "source": [
    "### Using The Model To Predict American Sign Language"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d41d2ea",
   "metadata": {},
   "source": [
    "Mostly using the same steps undertaken during Image Collection to preprocess the images in the ROI. But just adding the functionality of outputting the prediction and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b0b445d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'S'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.74711865"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "prevDisp = 'placeholder'\n",
    "while True:\n",
    "    _, frame = cap.read()\n",
    "    \n",
    "    mirror = cv2.flip(frame, 1)\n",
    "    \n",
    "    fh, fw = mirror.shape[:2]\n",
    "    rois = int(fh/1.7)    \n",
    "    cropImg = mirror[0:rois, fw-rois:fw]\n",
    "    \n",
    "    grey = cv2.cvtColor(cropImg, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    value = (7, 7)\n",
    "    blurred = cv2.GaussianBlur(grey, value, 0)\n",
    "    \n",
    "    _, thresh = cv2.threshold(blurred, 127, 255, cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)\n",
    "    \n",
    "    rgb = np.repeat(thresh[...,np.newaxis], 3, -1)\n",
    "    \n",
    "    mirror[0:rois, fw-rois:fw] = rgb\n",
    "    \n",
    "    resizedimg = cv2.resize(rgb, (224,224), interpolation= cv2.INTER_CUBIC)\n",
    "    \n",
    "    normalizedimgformodel = resizedimg/255.0\n",
    "    \n",
    "    with io.capture_output() as captured:\n",
    "        predictions = model.predict(np.array([normalizedimgformodel]))\n",
    "    if predictions.max()>0.7:\n",
    "        guessNo = np.argmax(np.squeeze(predictions))\n",
    "        guessAlpha = CATEGORIES[guessNo]\n",
    "        \n",
    "        if prevDisp != guessAlpha:\n",
    "            clear_output(wait=True)\n",
    "            display(guessAlpha, predictions.max())\n",
    "            prevDisp = guessAlpha\n",
    "            \n",
    "    cv2.imshow('WebCam', mirror)\n",
    "    if cv2.waitKey(20) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d465aeb",
   "metadata": {},
   "source": [
    "As we can see the model outputs the predicted handsign and the probability successfully. And from several tests the model performs pretty well with all the alphabets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
